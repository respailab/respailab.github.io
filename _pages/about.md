---
layout: about
title: Home
permalink: /
subtitle: "Welcome to the RespAI Lab!"

profile:
  align: left
  image: logo_respai.png
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>KIIT Bhubaneswar, India</p>

news: true # includes a list of news items
latest_posts: true # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Welcome to our research lab, led by [Dr. Murari Mandal](https://murarimandal.github.io/). At RespAI Lab, we focus on advancing large language models (LLMs) by addressing challenges related to long-content processing, inference efficiency, interpretability, and alignment. Our research also explores synthetic persona creation, regulatory issues, and innovative methods for model merging, knowledge verification, and unlearning.

<b>Motto of RespAI Lab</b>: Driving technical breakthroughs in AI through cutting-edge research and innovation, with a focus on solving complex challenges in LLMs and other generative models and contribute to top-tier conferences (ICML, ICLR, NeurIPS, AAAI, KDD, CVPR, ICCV, etc.) in the pursuit of cutting-edge advancements. 

<p><em>"When you go to hunt, hunt for rhino. If you fail, people will say anyway it was very difficult. If you succeed, you get all the glory"</em></p>

## <b>Ongoing Research at RespAI Lab</b>
- <b>Advanced Reasoning in LLMs:</b> Exploring techniques to enhance complex reasoning abilities in LLMs, focusing on multi-step problem solving, logical inference, and contextual understanding to improve decision-making and generate more reliable outcomes.

- <b>Addressing Challenges in Long-Content Processing for LLMs:</b> Investigating solutions to performance bottlenecks, memory limitations, latency issues, and information loss when dealing with extended content lengths in large language models (LLMs).
  
- <b>Optimizing LLM Inference Efficiency:</b> Developing strategies to reduce the computational cost of LLM inference, focusing on improving speed, memory usage, and leveraging smaller models for complex tasks.

- <b>Interpretability and Alignment of Generative AI Models:</b> Exploring the interpretability of generative AI models, aligning their outputs with human values, and addressing the issue of hallucinations in model responses.

- <b>Synthetic Persona and Society Creation:</b> Creating and studying synthetic personalities, communities, and societies within LLMs, and analyzing the behaviors and dynamics of these synthetic constructs.

- <b>Regulatory Challenges in LLMs:</b> Investigating regulatory concerns surrounding LLMs, including the implementation of unlearning techniques to comply with data privacy regulations and enhance model fairness.

- <b>Model Merging and Knowledge Verification:</b> Developing methods for merging multiple models, editing model behavior, and verifying the accuracy and consistency of the knowledge they generate.
